{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:05.847754Z",
     "start_time": "2024-08-19T19:46:05.840147Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from fleetrl.fleet_env.fleet_environment import FleetEnv\n",
    "from fleetrl.benchmarking.benchmark import Benchmark\n",
    "from fleetrl.benchmarking.uncontrolled_charging import Uncontrolled\n",
    "from fleetrl.benchmarking.distributed_charging import DistributedCharging\n",
    "from fleetrl.benchmarking.night_charging import NightCharging\n",
    "from fleetrl.benchmarking.linear_optimization import LinearOptimization\n",
    "\n",
    "from fleetrl.agent_eval.evaluation import Evaluation\n",
    "from fleetrl.agent_eval.basic_evaluation import BasicEvaluation\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, ProgressBarCallback, BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "\n",
    "from pink import PinkActionNoise\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise, NormalActionNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Copyright 2023 Enzo Alexander Cording - https://github.com/EnzoCording - GNU GPL v3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a computational approach to learning via interaction with an environment. It\n",
    "is a field of machine learning that can tackle complex problems by modelling them as\n",
    "decision-making processes. In RL, an agent interacts with an environment and seeks\n",
    "to achieve the optimal outcome by taking the right actions at the right time. Just like in\n",
    "the real world, there can be uncertainty or delayed consequences that require strategic\n",
    "behaviour. Trade-offs might have to be made between an immediate reward and a\n",
    "potential reward in the future. When an agent is first introduced to its environment, it\n",
    "does not possess the knowledge to evaluate its actions. It must therefore explore and\n",
    "learn through trial and error, as well as understand that some actions might only be\n",
    "beneficial at a later point in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline walks through the entire functionalities of FleetRL\n",
    "\n",
    "1) Creating a custom use-case\n",
    "    - Updating your data path\n",
    "    - Changing environment settings if needed\n",
    "    - Generating your own vehicle schedules\n",
    "2) Training an RL agent\n",
    "3) Building benchmark charging strategies\n",
    "4) Comparing the RL agent to the benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code could also be run in a .py file. Then, the code should be wrapped in:\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        #code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment: Simulation in which the algorithm takes actions, e.g. Mario Kart, or a Chess game\n",
    "Agent: The algorithm which learns how to make the best decisions over time\n",
    "Action: The decisions the agent takes\n",
    "Reward: Feedback signal from the environment, based on the agent's actions\n",
    "Policy: Strategy to get the best reward possible\n",
    "State: Representation of the environment, e.g. a frame of Mario Kart, the current Chess board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will be looking at a truck with the following parameters:\n",
    "\n",
    "- 600 kWh BESS\n",
    "- On board charger max power: 250 kW\n",
    "- Charging efficiency: 91%\n",
    "- Discharging efficiency: 91%\n",
    "- EV charger power: 120 kW\n",
    "\n",
    "More important info:\n",
    "- Model time resolution: 15 minutes\n",
    "- Energy calculations are from the perspective of the EV charger\n",
    "- 10 ct/kWh fixed markup\n",
    "- 50% electricity tax\n",
    "- 25% deducted from energy fed in to the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what a state looks like in FleetRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:13.561994Z",
     "start_time": "2024-08-19T19:46:13.159776Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takanori/FleetRL/fleetrl/utils/data_processing/data_processing.py:58: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  self.schedule = self.schedule.groupby(\"ID\").resample(time_conf.freq).agg(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'merged_time_left' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mFleetEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tutorial.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/FleetRL/fleetrl/fleet_env/fleet_environment.py:246\u001b[0m, in \u001b[0;36mFleetEnv.__init__\u001b[0;34m(self, env_config)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Loading the inputs\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader: DataLoader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspot_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtariff_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilding_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpv_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mev_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mev_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_soc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_building_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_pv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal_time\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# get the total database\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader\u001b[38;5;241m.\u001b[39mdb\n",
      "File \u001b[0;32m~/FleetRL/fleetrl/utils/data_processing/data_processing.py:67\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, path_name, schedule_name, spot_name, tariff_name, building_name, pv_name, time_conf, ev_conf, target_soc, building_flag, pv_flag, real_time)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# compute / preprocess from loaded schedule\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_from_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mev_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_soc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# create a date range with the chosen frequency\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Given the desired frequency, create a (down-sampled) column of timestamps\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_range \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m~/FleetRL/fleetrl/utils/data_processing/data_processing.py:193\u001b[0m, in \u001b[0;36mDataLoader.compute_from_schedule\u001b[0;34m(self, ev_conf, time_conf, target_soc)\u001b[0m\n\u001b[1;32m    190\u001b[0m merged_cons\u001b[38;5;241m.\u001b[39mloc[merged_cons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# fill NaN values\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m merged_cons \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_time_left\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_left\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# match departure dates with dates in db, forward direction, sort by ID, match on date\u001b[39;00m\n\u001b[1;32m    196\u001b[0m merged_time_left \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge_asof(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschedule\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    197\u001b[0m                                  res_departure\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    198\u001b[0m                                  on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    199\u001b[0m                                  by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m                                  direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m                                  )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'merged_time_left' referenced before assignment"
     ]
    }
   ],
   "source": [
    "env = FleetEnv(\"./tutorial.json\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is represented by the numpy array. It tells the agent what is currently going on with the EVs, the building load, spot price, etc.\n",
    "\n",
    "This is what the array contains - you can find this code under utils/observation/observer_bl_pv.py:\n",
    "\n",
    "    obs = {\n",
    "        \"soc\": list(soc),  # state of charge\n",
    "        \"hours_left\": list(hours_left),  # hours left at the charger\n",
    "        \"price\": list(price),  # price for charging\n",
    "        \"tariff\": list(tariff),  # price received when discharging\n",
    "        \"building_load\": list(building_load),  # building load in kW\n",
    "        \"pv\": list(pv),  # pv power in kW\n",
    "        \"there\": list(there),  # boolean, is the car i there or not\n",
    "        \"target_soc\": list(target_soc),  # target soc of car i\n",
    "        \"charging_left\": list(charging_left),  # charging % left\n",
    "        \"hours_needed\": list(hours_needed),  # hours needed to get to target soc\n",
    "        \"laxity\": list(laxity),  # laxity factor\n",
    "        \"evse_power\": list(evse_power),  # evse power in kW\n",
    "        \"grid_cap\": list(grid_cap),  # grid capacity\n",
    "        \"avail_grid_cap\": list(avail_grid_cap),  # available grid capacity\n",
    "        \"possible_avg_action\": list(possible_avg_action_per_car),  # possible avg action without overloading\n",
    "        \"month_sin\": month_sin,  # month in sin, and so on\n",
    "        \"month_cos\": month_cos,\n",
    "        \"week_sin\": week_sin,\n",
    "        \"week_cos\": week_cos,\n",
    "        \"hour_sin\": hour_sin,\n",
    "        \"hour_cos\": hour_cos\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we take an action in the environment.\n",
    "Actions can range from -1 to 1 - corresponding to the % of kW of the charger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:15.666644Z",
     "start_time": "2024-08-19T19:46:15.628191Z"
    }
   },
   "outputs": [],
   "source": [
    "env.step([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you just sent a charging signal with 120 kW over 15 minutes, totalling 30 kWh and the SOC went from 0.6012 to 0.647.\n",
    "That cost 2.52€ and since you spent money, you get a negative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:16.481175Z",
     "start_time": "2024-08-19T19:46:16.474878Z"
    }
   },
   "outputs": [],
   "source": [
    "new_soc = 30 * 0.91 / 600 + 0.6012\n",
    "price_with_fees = 0.084\n",
    "energy_drawn_at_charger = 30\n",
    "cost = energy_drawn_at_charger * price_with_fees\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try discharging the EV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:17.361619Z",
     "start_time": "2024-08-19T19:46:17.320180Z"
    }
   },
   "outputs": [],
   "source": [
    "env.step([-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the SOC went down to 0.597, as we take out 30 kWh from the battery, but only a portion of that arrives as useful energy at the charger. Selling at the spot market, we receive 1.273€ and thus a positive reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:18.311793Z",
     "start_time": "2024-08-19T19:46:18.304931Z"
    }
   },
   "outputs": [],
   "source": [
    "new_soc = 0.647 - 30/600\n",
    "useful_energy = 30*0.91\n",
    "deduction = 0.25\n",
    "revenue = useful_energy * (0.062 * (1-deduction))\n",
    "revenue  # difference due to rounding of spot price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalties are given out for invalid actions, e.g. overcharging the battery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:19.309867Z",
     "start_time": "2024-08-19T19:46:18.978538Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    env.step([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, over charging the battery has a small penalty, but overloading the grid connection and potentially blowing a fuse has much larger implications and thus a much bigger penalty. The agent learns over time to avoid grid overloading at all costs, and then focuses on minimising small penalties such as this one. Penalties are also incurred for letting the car leave without reaching target SOC of 85%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:23.603798Z",
     "start_time": "2024-08-19T19:46:20.174292Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(31*4):\n",
    "    env.step([-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A car left the station without reaching the target SoC. Penalty: -498.949\n",
    "Reward signal: -503.899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have got a hang of the basics, let's configure our own use-case and train an agent on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a custom use-case**\n",
    "Go through this step by step and check the documentation if needed. The docs specify what type of input data is required, what format it should be in, etc.\n",
    "The code below is commented to provide the most essential information.\n",
    "\n",
    "Docs: fleetrl.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General settings**\n",
    "Under general settings, you can adjust how many vehicles to optimize for, whether you would like to create new schedules how long the episodes should be, etc.\n",
    "There is a pre-trained agent for the 1-EV environments. So you can train your own agent and compare its performance. There are also benchmarks of uncontrolled charging, night charging, and linear optimisation to compare against deterministic charging strategies that are being used today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more important concepts:\n",
    "\n",
    "Episode: One iteration of the agent through the environment, until a final state is reached or the time limit expires - e.g. a game of chess or in this case, a certain number of consecutive hours of managing the EV charging process\n",
    "\n",
    "Training: When an episode is done, the agent takes the collected experience, learns from it and starts a new episode. This process can go on as long as you like. Typically, you observe the agent's performance and interrupt training as soon as performance begins to pleateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:28.818184Z",
     "start_time": "2024-08-19T19:46:28.813198Z"
    }
   },
   "outputs": [],
   "source": [
    "# define fundamental parameters\n",
    "# data path to inputs folder with schedule csv, prices, load, pv, etc.\n",
    "input_data_path: str = \"inputs\"  # path to input folder, starting from the location of this jupyter notebook\n",
    "time_now = int(time.time())\n",
    "run_name: str = f\"Test_run_custom_{time_now}\"  # Change this name or make it dynamic, e.g. with a timestamp\n",
    "n_train_steps = 48  # number of hours in a training episode\n",
    "n_eval_steps = 168  # number of hours in one evaluation episode\n",
    "n_eval_episodes = 1  # number of episodes for evaluation\n",
    "n_evs = 1  # number of evs\n",
    "n_envs = 2  # number of environment running in parallel (speeds up training, 1 env = 1 CPU)\n",
    "time_steps_per_hour = 4  # temporal resolution of the simulation (quarter-hourly)\n",
    "use_case: str = \"custom\"  # for file name - lmd=last mile delivery, by default can insert \"lmd\", \"ct\", \"ut\", \"custom\"\n",
    "custom_schedule_name = \"1_lkw.csv\"  # name for custom schedule if you have generated one. If you want to generate one this time, this field will be ignored\n",
    "scenario: Literal[\"arb\", \"tariff\"] = \"arb\"  # arbitrage or tariff. Arbitrage allows for bidirectional spot trading, no fees. Tariff models commercial tariff with grid fees, electricity tax, etc.\n",
    "gen_new_schedule = False  # generate a new schedule - refer to schedule generator documentation and adjust statistics in config.json\n",
    "gen_new_test_schedule = False  # generate a new schedule for agent testing\n",
    "end_cutoff = 330  # the dataset has 365 days. We want to only train on 1 month of data in this lab, so we remove the last 11 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:29.605557Z",
     "start_time": "2024-08-19T19:46:29.602051Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will be simulating a truck\n",
    "ev_charger_power = 130  # kW\n",
    "grid_connection_limit = 500  # kW\n",
    "battery_size = 600  # kWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training settings**\n",
    "These more low-level settings allow you to change training-related parameters. Refer to the documentation of FleetRL and stable-baselines3 for further details. Observations are by default normalized within SB3, due to their rolling average normalization. You can also conduct absolute normalization via FleetRL. (In that case, the observation of electricity price is divided by the global maximum electricity price in the dataset). Normalization is common practice when working with neural networks for numeric stability, faster training process, and because neural network optimisers work best with normalized numbers.\n",
    "\n",
    "Adapt total training steps and saving interval for a full run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:30.707972Z",
     "start_time": "2024-08-19T19:46:30.704353Z"
    }
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "norm_obs_in_env = False  # normalize observations within FleetRL (max, min normalization)\n",
    "vec_norm_obs = True  # normalize observations in SB3 (rolling normalization)\n",
    "vec_norm_rew = True  # normalize rewards in SB3 (rolling normalization)\n",
    "\n",
    "# Total steps should be sep to 1e6 or 5e6 for a full run. Check tensorboard for stagnating reward signal and stop training at some point to avoid overfit\n",
    "total_steps = int(5e3)  # total training time steps\n",
    "\n",
    "# Specifies how often you want to make an intermediate artifact. For a full run, I recommend every 50k - 100k steps, so you can backtrack for best model\n",
    "saving_interval = 5e2  # interval for saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters for environment object creation**\n",
    "Further settings can be adjusted below, view the comments and docs for more detailed explanations.\n",
    "Most important:\n",
    "- Episode length: How long an episode is in hours - at least 36 hours are recommended so the agent always sees one passage of a night for night charging\n",
    "- include_building, include_pv, include_price: These adjust the shape of the observations to make the problem simpler or more complex\n",
    "- price_lookahead, bl_pv_lookahead: These dictate how much knowledge into the future the agent has on price, building load and PV in hours\n",
    "- Time picker: Use random during training: This way, a new episode always starts at a random point in the dataframe\n",
    "- Deg_emp: For simple degradation, set to True\n",
    "- Ignore_x_reward: Set accordingly with Include_x... or deactivate certain parts of the reward function to adjust problem complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:31.753661Z",
     "start_time": "2024-08-19T19:46:31.736841Z"
    }
   },
   "outputs": [],
   "source": [
    "# environment arguments - adjust settings if necessary\n",
    "# additional settings can be changed in the config files\n",
    "env_config = {\"data_path\": input_data_path,\n",
    "              # Specify file names: there is a naming convention for default files, otherwise, custom name is used\n",
    "              \"schedule_name\": (str(n_evs) + \"_\" + str(use_case) + \".csv\") if use_case != \"custom\" else custom_schedule_name,\n",
    "              \"building_name\": \"load_\" + str(use_case) + \".csv\" if use_case != \"custom\" else \"load_lmd.csv\",\n",
    "              \"pv_name\": None,  # if separate file for PV inputs, specify here, otherwise, uses \"PV\" column in building_name\n",
    "              # Define use case\n",
    "              \"use_case\": use_case,\n",
    "              # Change observation space\n",
    "              \"include_building\": True,  # False removes building load from Observation\n",
    "              \"include_pv\": True,  # False removes PV from Observation\n",
    "              \"include_price\": True,  # False removes electricity prices from Observation\n",
    "              \"price_lookahead\": 8,  # Hours seen into the future price\n",
    "              \"bl_pv_lookahead\": 4,  # Hours seen into the future building load and pv\n",
    "              \"time_steps_per_hour\": 4,  # Time resolution\n",
    "              # Specify time picker: \"eval\", \"static\", or \"random\" are implemented\n",
    "              \"time_picker\": \"random\",  # Pick a random starting day in the schedule dataframe\n",
    "              \"end_cutoff\": end_cutoff,  # how many days to remove from the end of the dataset\n",
    "              # Pick degradation methodology: True sets empirical degradation from real measurements\n",
    "              \"deg_emp\": False,  # empirical degradation calculation\n",
    "              # Shape reward function\n",
    "              \"ignore_price_reward\": False,  # True sets price-related reward coefficient to 0\n",
    "              \"ignore_invalid_penalty\": False,  # True ignores penalties on invalid actions (charging an empty spot)\n",
    "              \"ignore_overcharging_penalty\": False,  # True ignores penalties on charging signals above target SOC\n",
    "              \"ignore_overloading_penalty\": False,  # True ignores grid connection overloading penalty\n",
    "              # Set episode length during training\n",
    "              \"episode_length\": n_train_steps,  # in hours\n",
    "              # Additional parameters\n",
    "              \"normalize_in_env\": norm_obs_in_env,  # Conduct normalization within FleetRL.\n",
    "              \"verbose\": 0,  # Print statements, can slow down FPS\n",
    "              \"aux\": True,  # Include auxiliary data (recommended). Check documentation for more information.\n",
    "              \"log_data\": False,  # Log data (Makes most sense for evaluation runs)\n",
    "              \"calculate_degradation\": True,  # Calculate SOH degradation (Can slow down FPS)\n",
    "              # Target SOC\n",
    "              \"target_soc\": 0.85,  # Signals that would charge above target SOC are clipped.\n",
    "              # settings regarding the generation of evs\n",
    "              # seed for random number generation\n",
    "              \"seed\": 42,  # Seed for RNG - can be set to None so always random (not recommended)\n",
    "              # if you are comparing cars with different bess sizes, use this to norm their reward function range\n",
    "              \"max_batt_cap_in_all_use_cases\": 600,\n",
    "              \"init_battery_cap\": 600,\n",
    "              # initial state of health of the battery\n",
    "              \"init_soh\": 1.0,\n",
    "              \"min_laxity\": 1.75,\n",
    "              \"obc_max_power\": 250,\n",
    "              \"real_time\": False,\n",
    "              # settings below if you want to generate new schedules\n",
    "              \"gen_schedule\": gen_new_schedule,  # boolean to generate a new schedule\n",
    "              \"gen_start_date\": \"2021-01-01 00:00\",  # if new schedule, start date\n",
    "              \"gen_end_date\": \"2021-12-31 23:59:59\",  # if new schedule, end date\n",
    "              \"gen_name\": \"my_custom_schedule.csv\",  # name of newly generated schedule\n",
    "              \"gen_n_evs\": 1,  # number of EVs in new schedule, per EV it takes ca. 10-20 min.\n",
    "              # custom schedule timing settings, mean and standard deviation\n",
    "              \"custom_weekday_departure_time_mean\": 7,\n",
    "              \"custom_weekday_departure_time_std\": 1,\n",
    "              \"custom_weekday_return_time_mean\": 19,\n",
    "              \"custom_weekday_return_time_std\": 1,\n",
    "              \"custom_weekend_departure_time_mean\": 9,\n",
    "              \"custom_weekend_departure_time_std\": 1.5,\n",
    "              \"custom_weekend_return_time_mean\": 17,\n",
    "              \"custom_weekend_return_time_std\": 1.5,\n",
    "              \"custom_earliest_hour_of_departure\": 3,\n",
    "              \"custom_latest_hour_of_departure\": 11,\n",
    "              \"custom_earliest_hour_of_return\": 12,\n",
    "              \"custom_latest_hour_of_return\": 23,\n",
    "              # custom distance settings\n",
    "              \"custom_weekday_distance_mean\": 300,\n",
    "              \"custom_weekday_distance_std\": 25,\n",
    "              \"custom_weekend_distance_mean\": 150,\n",
    "              \"custom_weekend_distance_std\": 25,\n",
    "              \"custom_minimum_distance\": 20,\n",
    "              \"custom_max_distance\": 400,\n",
    "              # custom consumption data for vehicle\n",
    "              \"custom_consumption_mean\": 1.3,\n",
    "              \"custom_consumption_std\": 0.167463672468669,\n",
    "              \"custom_minimum_consumption\": 0.3,\n",
    "              \"custom_maximum_consumption\": 2.5,\n",
    "              \"custom_maximum_consumption_per_trip\": 500,\n",
    "              # custom ev-related settings\n",
    "              \"custom_ev_charger_power_in_kw\": ev_charger_power,\n",
    "              \"custom_ev_battery_size_in_kwh\": battery_size,\n",
    "              \"custom_grid_connection_in_kw\": grid_connection_limit\n",
    "              }\n",
    "\n",
    "# commercial tariff scenario, fixed fee on spot price (+10 ct/kWh, and a 50% mark-up)\n",
    "# Feed-in tariff orientates after PV feed-in, with 25% deduction\n",
    "if scenario == \"tariff\":\n",
    "    env_config[\"spot_markup\"] = 10\n",
    "    env_config[\"spot_mul\"] = 1.5\n",
    "    env_config[\"feed_in_ded\"] = 0.25\n",
    "    env_config[\"price_name\"] = \"spot_2021_new.csv\"\n",
    "    env_config[\"tariff_name\"] = \"fixed_feed_in.csv\"\n",
    "\n",
    "# arbitrage scenario, up and down prices are spot price, no markups or taxes\n",
    "elif scenario == \"arb\":\n",
    "    env_config[\"spot_markup\"] = 0\n",
    "    env_config[\"spot_mul\"] = 1\n",
    "    env_config[\"feed_in_ded\"] = 0\n",
    "    env_config[\"price_name\"] = \"spot_2021_new.csv\"\n",
    "    env_config[\"tariff_name\"] = \"spot_2021_new_tariff.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save this config and re-use it or share it with others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:46:37.992780Z",
     "start_time": "2024-08-19T19:46:37.987844Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"my_config_v1.json\", \"w\") as file:\n",
    "    json.dump(env_config, file, indent=4)\n",
    "    \n",
    "# to load it:\n",
    "# with open(\"my_config_v1.json\", \"r\") as file:\n",
    "#     my_conf = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment object creation**\n",
    "Vec_Env are created to enable multi-processing.\n",
    "\n",
    "Train_vec_env: For agent training\n",
    "Eval_vec_env: For agent evaluation during training on same csv file (70% training data, 30% evaluation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:09.249038Z",
     "start_time": "2024-08-19T19:50:02.354541Z"
    }
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\"env_config\": env_config}\n",
    "\n",
    "train_vec_env = make_vec_env(FleetEnv,\n",
    "                             n_envs=n_envs,\n",
    "                             vec_env_cls=SubprocVecEnv,\n",
    "                             env_kwargs=env_kwargs,\n",
    "                             seed=env_config[\"seed\"])\n",
    "\n",
    "train_norm_vec_env = VecNormalize(venv=train_vec_env,\n",
    "                                  norm_obs=vec_norm_obs,\n",
    "                                  norm_reward=vec_norm_rew,\n",
    "                                  training=True,\n",
    "                                  clip_reward=10.0)\n",
    "\n",
    "env_config[\"time_picker\"] = \"eval\"\n",
    "\n",
    "if gen_new_schedule:\n",
    "    env_config[\"gen_schedule\"] = False\n",
    "    env_config[\"schedule_name\"] = env_config[\"gen_name\"]\n",
    "\n",
    "env_kwargs = {\"env_config\": env_config}\n",
    "\n",
    "eval_vec_env = make_vec_env(FleetEnv,\n",
    "                            n_envs=n_envs,\n",
    "                            vec_env_cls=SubprocVecEnv,\n",
    "                            env_kwargs=env_kwargs,\n",
    "                            seed=env_config[\"seed\"])\n",
    "\n",
    "eval_norm_vec_env = VecNormalize(venv=eval_vec_env,\n",
    "                                  norm_obs=vec_norm_obs,\n",
    "                                  norm_reward=vec_norm_rew,\n",
    "                                  training=True,\n",
    "                                  clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a schedule for testing the trained agents on unseen data.\n",
    "It is recommended to generate a testing schedule along with a newly generated training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:15.502026Z",
     "start_time": "2024-08-19T19:50:12.244487Z"
    }
   },
   "outputs": [],
   "source": [
    "if gen_new_test_schedule:\n",
    "    # generate an evaluation schedule\n",
    "    test_sched_name = env_config[\"gen_name\"]\n",
    "    if not test_sched_name.endswith(\".csv\"):\n",
    "        test_sched_name = test_sched_name + \"_test\" + \".csv\"\n",
    "    else:\n",
    "        test_sched_name = test_sched_name.strip(\".csv\")\n",
    "        test_sched_name = test_sched_name + \"_test\" + \".csv\"\n",
    "\n",
    "    env_config[\"gen_schedule\"] = True\n",
    "    env_config[\"gen_name\"] = test_sched_name\n",
    "\n",
    "    env_kwargs = {\"env_config\": env_config}\n",
    "\n",
    "    test_vec_env = make_vec_env(FleetEnv,\n",
    "                                n_envs=1,\n",
    "                                vec_env_cls=SubprocVecEnv,\n",
    "                                env_kwargs=env_kwargs,\n",
    "                                seed=env_config[\"seed\"])\n",
    "\n",
    "    env_config[\"gen_schedule\"] = False\n",
    "    env_config[\"schedule_name\"] = test_sched_name\n",
    "\n",
    "    env_kwargs = {\"env_config\": env_config}\n",
    "\n",
    "test_vec_env = make_vec_env(FleetEnv,\n",
    "                            n_envs=n_envs,\n",
    "                            vec_env_cls=SubprocVecEnv,\n",
    "                            env_kwargs=env_kwargs,\n",
    "                            seed=env_config[\"seed\"])\n",
    "\n",
    "test_norm_vec_env = VecNormalize(venv=test_vec_env,\n",
    "                                 norm_obs=vec_norm_obs,\n",
    "                                 norm_reward=vec_norm_rew,\n",
    "                                 training=True,\n",
    "                                 clip_reward=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are regularly called during training and enable useful functionalities such as logging or progress reporting. View SB3 docs for further information. Note that wandb callbacks are possible with SB3.\n",
    "\n",
    "Eval callback triggers an evaluation at fixed intervals\n",
    "HyperParamCallback logs hyperparameters, also visible in TensorBoard\n",
    "ProgressBar indicated progress of an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:17.178396Z",
     "start_time": "2024-08-19T19:50:17.171937Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(eval_env=eval_norm_vec_env,\n",
    "                             warn=True,\n",
    "                             verbose=1,\n",
    "                             deterministic=True,\n",
    "                             eval_freq=max(10000 // n_envs, 1),\n",
    "                             n_eval_episodes=5,\n",
    "                             render=False,\n",
    "                             )\n",
    "\n",
    "class HyperParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves hyperparameters and metrics at start of training, logging to tensorboard\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "\n",
    "        metric_dict = {\n",
    "            \"rollout/ep_len_mean\": 0,\n",
    "            \"train/value_loss\": 0.0,\n",
    "        }\n",
    "\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\")\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "progress_bar = ProgressBarCallback()\n",
    "\n",
    "## wandb callback possible, check documentation of SB3 and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:17.812466Z",
     "start_time": "2024-08-19T19:50:17.809206Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameter_callback = HyperParamCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If you use TD3, pink action noise is said to improve performance\n",
    "If you use PPO, this is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:19.033345Z",
     "start_time": "2024-08-19T19:50:19.028670Z"
    }
   },
   "outputs": [],
   "source": [
    "# model-related settings\n",
    "n_actions = train_norm_vec_env.action_space.shape[-1]\n",
    "param_noise = None\n",
    "noise_scale = 0.1\n",
    "seq_len = n_train_steps * time_steps_per_hour\n",
    "action_noise = PinkActionNoise(noise_scale, seq_len, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can choose to create your own agent of load an existing one. A pretrained PPO agent exists for a 1 EV environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:20.950904Z",
     "start_time": "2024-08-19T19:50:20.863566Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "load_existing_agent = True\n",
    "if load_existing_agent:\n",
    "    model = PPO.load(path=\"./rl_agents/trained_agents/LMD_arbitrage_1e6_steps_example_agent/PPO-fleet_LMD_2021_arbitrage_PPO_mul3.zip\", env=train_norm_vec_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If you want to use a loaded agent, you can skip the next 5 cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:22.797018Z",
     "start_time": "2024-08-19T19:50:22.792692Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if not load_existing_agent:\n",
    "    model = PPO(policy=\"MlpPolicy\",\n",
    "                verbose=env_config[\"verbose\"], # setting verbose to 0 can introduce performance increases in jupyterlab environments\n",
    "                env=train_norm_vec_env,\n",
    "                tensorboard_log=\"./rl_agents/trained_agents/tb_log\")\n",
    "\n",
    "# might introduce performance increases\n",
    "            # gamma=0.99,\n",
    "            # learning_rate=0.0005,\n",
    "            # batch_size=128,\n",
    "            # n_epochs=8,\n",
    "            # gae_lambda=0.9,\n",
    "            # clip_range=0.2,\n",
    "            # clip_range_vf=None,\n",
    "            # normalize_advantage=True,\n",
    "            # ent_coef=0.0008,\n",
    "            # vf_coef=0.5,\n",
    "            # max_grad_norm=0.5,\n",
    "            # n_steps=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:27.559394Z",
     "start_time": "2024-08-19T19:50:25.037163Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: make the Notebook trusted\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./rl_agents/trained_agents/tb_log --port 6006 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:52.453202Z",
     "start_time": "2024-08-19T19:50:52.448773Z"
    }
   },
   "outputs": [],
   "source": [
    "if not load_existing_agent:\n",
    "    comment = run_name\n",
    "    time_now = int(time.time())\n",
    "    trained_agents_dir = f\"./rl_agents/trained_agents/vec_PPO_{time_now}_{run_name}\"\n",
    "    logs_dir = f\"{trained_agents_dir}/logs/\"\n",
    "    \n",
    "    if not os.path.exists(trained_agents_dir):\n",
    "        os.makedirs(trained_agents_dir)\n",
    "    \n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:50:53.862588Z",
     "start_time": "2024-08-19T19:50:53.856084Z"
    }
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "# models are saved in a specified interval: once with unique step identifiers\n",
    "# model and the normalization metrics are saved as well, overwriting the previous file every time\n",
    "if not load_existing_agent:\n",
    "    for i in range(0, int(total_steps / saving_interval)):\n",
    "        print(f\"iteration: {i}\")\n",
    "        model.learn(total_timesteps=saving_interval,\n",
    "                    reset_num_timesteps=False,\n",
    "                    tb_log_name=f\"PPO_{time_now}_{comment}\",\n",
    "                    callback=[eval_callback, hyperparameter_callback, progress_bar])\n",
    "    \n",
    "        model.save(f\"{trained_agents_dir}/{saving_interval * i}\")\n",
    "    \n",
    "        # Don't forget to save the VecNormalize statistics when saving the agent\n",
    "        tmp_dir = f\"{trained_agents_dir}/tmp/\"\n",
    "        model_path = tmp_dir + f\"PPO-fleet_{comment}_{time_now}\"\n",
    "        model.save(model_path)\n",
    "        stats_path = os.path.join(tmp_dir, f\"vec_normalize-{comment}_{time_now}.pkl\")\n",
    "        train_norm_vec_env.save(stats_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Agent evaluation\n",
    "- Sets the time_picker to static, so an evaluation always starts at the same point in time\n",
    "- Usually at the beginning of the year and then runs for a long time, e.g. a whole year to test the agent's ability to handle large amounts of unseen data\n",
    "- Make sure that the environment passed to the trained agent includes the unseen schedule\n",
    "- For that you should use test_norm_vec_env with the _test.csv schedule\n",
    "\n",
    "- Data is logged to be used for data analytics later. Every datapoint during the evaluation is tracked at every timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:52:14.727306Z",
     "start_time": "2024-08-19T19:52:14.719001Z"
    }
   },
   "outputs": [],
   "source": [
    "# environment arguments for evaluation\n",
    "env_config[\"time_picker\"] = \"static\"  # Pick a random starting day in the schedule dataframe\n",
    "env_config[\"log_data\"] = True,  # Log data (Makes most sense for evaluation runs)\n",
    "\n",
    "env_kwargs = {\"env_config\": env_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Creation of Evaluation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:52:16.107652Z",
     "start_time": "2024-08-19T19:52:16.095505Z"
    }
   },
   "outputs": [],
   "source": [
    "eval: Evaluation = BasicEvaluation(n_steps=n_eval_steps,\n",
    "                                   n_evs=n_evs,\n",
    "                                   n_episodes=n_eval_episodes,\n",
    "                                   n_envs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "- Use this is you are using a loaded agent. Otherwise, skip this cell\n",
    "- If you have just trained an agent, stats_path and model_path have been automatically defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:52:18.629435Z",
     "start_time": "2024-08-19T19:52:18.626228Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if load_existing_agent:\n",
    "    stats_path = \"./rl_agents/trained_agents/LMD_arbitrage_1e6_steps_example_agent/vec_normalize-LMD_2021_arbitrage_PPO_mul3.pkl\"\n",
    "    model_path = \"./rl_agents/trained_agents/LMD_arbitrage_1e6_steps_example_agent/PPO-fleet_LMD_2021_arbitrage_PPO_mul3.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Saves the log from agent evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:52:39.579855Z",
     "start_time": "2024-08-19T19:52:19.909396Z"
    }
   },
   "source": [
    "## steps through a week of data and calls the agent's learnt strategy\n",
    "rl_log = eval.evaluate_agent(env_kwargs=env_kwargs, norm_stats_path=stats_path, model_path=model_path, seed=env_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The cells below define benchmarks and run them, saving the logs in the same way as RL evaluation\n",
    "- Uncontrolled charging: Plug in on arrival\n",
    "- LP: Linear-programming model - requires an LP solver (on Windows you might run into trouble when doing pip install glpk or similar due to C++ or MSVC. In that case you can try using a Conda environment for the repo and it should work. View github tutorial)\n",
    "  - Gurobi requires a license to be specified\n",
    "- Dist: Distributed charging, charges such that the car is completely filled up the moment before it leaves, charging the whole time with evenly distributed power\n",
    "- Night: Charging at night only at set times, e.g. from 1am to 5am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:53:33.651874Z",
     "start_time": "2024-08-19T19:53:14.651445Z"
    }
   },
   "outputs": [],
   "source": [
    "uncontrolled_charging: Benchmark = Uncontrolled(n_steps=n_eval_steps,\n",
    "                                                n_evs=n_evs,\n",
    "                                                n_episodes=n_eval_episodes,\n",
    "                                                n_envs=1,\n",
    "                                                time_steps_per_hour=time_steps_per_hour)\n",
    "\n",
    "uc_log = uncontrolled_charging.run_benchmark(env_kwargs=env_kwargs, use_case=use_case, seed=env_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To try out linear optimisation, glpk must be installed. Alternatively, you can use your gurobi license. Simply swap out \"glpk\" for \"gurobi\" in linear_optimization.py in line 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:53:56.355066Z",
     "start_time": "2024-08-19T19:53:34.560891Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lp: Benchmark = LinearOptimization(n_steps=n_eval_steps,\n",
    "                                   n_evs=n_evs,\n",
    "                                   n_episodes=n_eval_episodes,\n",
    "                                   n_envs=1,\n",
    "                                   time_steps_per_hour=time_steps_per_hour)\n",
    "\n",
    "lp_log = lp.run_benchmark(env_kwargs=env_kwargs, use_case=use_case, seed=env_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:54:28.778651Z",
     "start_time": "2024-08-19T19:53:58.976819Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dist: Benchmark = DistributedCharging(n_steps=n_eval_steps, n_evs=n_evs, n_episodes=n_eval_episodes, n_envs=1, time_steps_per_hour=time_steps_per_hour)\n",
    "\n",
    "dist_log = dist.run_benchmark(env_kwargs=env_kwargs, use_case=use_case, seed=env_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:54:54.257794Z",
     "start_time": "2024-08-19T19:54:34.155881Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "night: Benchmark = NightCharging(n_steps=n_eval_steps, n_evs=n_evs, n_episodes=n_eval_episodes, n_envs=1, time_steps_per_hour=time_steps_per_hour)\n",
    "\n",
    "night_log = night.run_benchmark(env_kwargs=env_kwargs, use_case=use_case, seed=env_config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can plot the average actions of the benchmarks. Just provide the log dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:56:24.777952Z",
     "start_time": "2024-08-19T19:56:24.252517Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lp.plot_benchmark(lp_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:56:28.862866Z",
     "start_time": "2024-08-19T19:56:28.611618Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "uncontrolled_charging.plot_benchmark(uc_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This showcases some evaluation methods that have already been written for easy comparison\n",
    "You can, of course, create new analytics from the log dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:56:34.802003Z",
     "start_time": "2024-08-19T19:56:31.465192Z"
    }
   },
   "outputs": [],
   "source": [
    "eval.compare(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.compare(rl_log=rl_log, benchmark_log=lp_log)\n",
    "eval.plot_soh(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.plot_soh(rl_log=rl_log, benchmark_log=dist_log)\n",
    "eval.plot_soh(rl_log=rl_log, benchmark_log=night_log)\n",
    "eval.plot_violations(rl_log=rl_log, benchmark_log=uc_log)\n",
    "eval.plot_action_dist(rl_log=rl_log, benchmark_log=uc_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The detailed plot shows what is happening during certain days of the simulation. You can add as many benchmark logs as you have compiled, the figure will automatically adjust based on number of EVs, number of logs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T19:56:48.899797Z",
     "start_time": "2024-08-19T19:56:48.207040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "detailed_fig = eval.plot_detailed_actions(start_date=\"2021-01-02 19:00\", \n",
    "                                          end_date=\"2021-01-04 18:45\",\n",
    "                                          rl_log=rl_log,\n",
    "                                          lp_log=lp_log,\n",
    "                                          uc_log=uc_log,\n",
    "                                          dist_log=dist_log,\n",
    "                                          night_log=night_log)\n",
    "\n",
    "detailed_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the log files - can you figure out how much money was spent for each charging strategy?\n",
    "Below is a function that can help you filter the observation list and extract important information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:06:29.765273Z",
     "start_time": "2024-08-19T20:06:29.733534Z"
    }
   },
   "outputs": [],
   "source": [
    "rl_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:06:13.052892Z",
     "start_time": "2024-08-19T20:06:13.042848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_from_obs(log: dict):\n",
    "\n",
    "    obs = log[\"Observation\"]\n",
    "    act = log[\"Charging energy\"]\n",
    "    cf = log[\"Cashflow\"]\n",
    "    env_config = env_kwargs[\"env_config\"]\n",
    "\n",
    "    bl_pv_lookahead = env_config[\"bl_pv_lookahead\"]\n",
    "    pr_lookahead = env_config[\"price_lookahead\"]\n",
    "\n",
    "    length = len(log)\n",
    "\n",
    "    # Check observer class to see how observation list is built up\n",
    "\n",
    "    date = log[\"Time\"]\n",
    "    first = 0  # first entry has index 0\n",
    "    last = n_evs - 1  # soc for each car\n",
    "    if n_evs > 1:\n",
    "        soc = [obs[i][first:last].mean() for i in range(length)]\n",
    "    else:\n",
    "        soc = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    first = n_evs\n",
    "    last = n_evs * 2 - 1  # hours left at charger for each car\n",
    "    if n_evs > 1:\n",
    "        hours_left = [obs[i][first:last].mean() for i in range(length)]\n",
    "    else:\n",
    "        hours_left = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    first = n_evs * 2\n",
    "    last = n_evs * 2 + pr_lookahead  # price lookahead gives price in hour, hour+1, etc.\n",
    "    price = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    first = n_evs * 2 + pr_lookahead + 1\n",
    "    last = n_evs * 2 + pr_lookahead * 2 + 1  # tariff paid when discharging, with lookahead\n",
    "    tariff = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    first = n_evs * 2 + pr_lookahead * 2 + 2\n",
    "    last = n_evs * 2 + pr_lookahead * 2 + 2 + bl_pv_lookahead  # building load lookahead\n",
    "    building_load = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    first = n_evs * 2 + pr_lookahead * 2 + 2 + bl_pv_lookahead + 1\n",
    "    last = n_evs * 2 + pr_lookahead * 2 + bl_pv_lookahead * 2 + 1  # pv has same lookahead as building\n",
    "    pv = [obs[i][first] for i in range(length)]\n",
    "\n",
    "    free_cap = [obs[i][-8] / obs[i][-9] for i in range(length)]  # free grid capacity in MW / total grid capacity\n",
    "\n",
    "    time_steps_per_hour = env_config[\"time_steps_per_hour\"]\n",
    "\n",
    "    # act is charging energy in kWh, we want to display the currently drawn power in kW\n",
    "    first = 0  # first entry has index 0\n",
    "    last = n_evs - 1  # soc for each car\n",
    "    if n_evs > 1:\n",
    "        action = [act[i][first:last].sum() * time_steps_per_hour for i in range(length)]  # Going from kWh to kW\n",
    "    else:\n",
    "        action = [act[i][first] * time_steps_per_hour for i in range(length)]  # Going from kWh to kW\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Date': date,\n",
    "        'SOC': soc,\n",
    "        'Load': building_load,\n",
    "        'PV': pv,\n",
    "        'Price': price,\n",
    "        'Action': action,\n",
    "        'Free cap': free_cap,\n",
    "        'CF': cf\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:06:14.328667Z",
     "start_time": "2024-08-19T20:06:14.289202Z"
    }
   },
   "outputs": [],
   "source": [
    "get_from_obs(rl_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:06:23.756740Z",
     "start_time": "2024-08-19T20:06:23.754011Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
